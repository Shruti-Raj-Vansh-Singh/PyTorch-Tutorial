{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkXg1jlKUW/Ypy4lM82s1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti-Raj-Vansh-Singh/PyTorch-Tutorial/blob/master/PyTorch_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKyelsM3zzA",
        "colab_type": "text"
      },
      "source": [
        "# AUTOGRAD\n",
        "It provides automatic differentiation for all operations on Tensors. \\\n",
        "It is a** define-and-run framework** which means that the backprop defined by how we code is run and that every single iteration can be different "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdwvGun3krP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i1U7MzR-L8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a020fe0b-7c3a-420d-cfae-a3a3f64d7cc1"
      },
      "source": [
        "#creating a tensor with require_grad=true so that we can track the computations within\n",
        "x = torch.ones(2,2,requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy_f4utd-baa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d2bc5db5-0e5a-4d43-f1cc-174d3d242ecf"
      },
      "source": [
        "#tensor operation\n",
        "y = x+2\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWAdyMaARJD",
        "colab_type": "text"
      },
      "source": [
        "grad_fn refers a function that has created the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP1aW6aF-k4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33fbf058-835c-4b4f-93a7-c4402131722b"
      },
      "source": [
        "#since y was created as an operation function for x, it has a grad_fn\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7f93e7ad1c18>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJduvMQV-9AD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "095b79a6-d658-4d88-fb1b-f8c2196b5d33"
      },
      "source": [
        "z = y*y*3\n",
        "out = y.mean()\n",
        "print(z, out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhqsaovv_dCx",
        "colab_type": "text"
      },
      "source": [
        ".requires_grad_() changes an existing tensor's requires_grad flag in-place. The input flag defaults to False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8NDhAeg_U_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "04fe42b2-24f9-49cc-a4ac-f277940a248f"
      },
      "source": [
        "a = torch.rand(2,2)\n",
        "a = ((a-3)/(a-1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b =  (a*a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7f93e7b3a630>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2wAmfviAi4K",
        "colab_type": "text"
      },
      "source": [
        "# GRADIENTS\n",
        "backprop \\\n",
        "since out is a single tensor we can directly write out.backward() instead of out.backward(torch.tensor(1.))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGr00DF-AC-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NgWx0jKBBoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ad1b99f1-9b57-4abd-8146-41ade4412d3d"
      },
      "source": [
        "#printing the gradient\n",
        "print(x.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SLPv7IB6WJ",
        "colab_type": "text"
      },
      "source": [
        "Generally speaking, torch.autograd is an engine for computing vector-Jacobian product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKVUaYX-BG7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44f97e15-a237-46ab-d165-f29a934ffa66"
      },
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "y = x*3\n",
        "while y.data.norm() <1000:\n",
        "  y=y*2\n",
        "print(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 318.8128,  135.3183, 1363.8234], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAvUY9OuCRWT",
        "colab_type": "text"
      },
      "source": [
        "in this case y is no longer a scalar. hence torch.autograd cannot compute the full jacobin directly, but if we want the vector jacobian prodeuct we simply need to pass backward to y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk0Ewa1pCHK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f95b0633-9e9f-4c74-b9c1-e0912326fe6e"
      },
      "source": [
        "v = torch.tensor([0.1,1.0,0.0001], dtype = float)\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.5360e+02, 1.5360e+03, 1.5360e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8a9VkhD6Yx",
        "colab_type": "text"
      },
      "source": [
        "if we dont want autograd to keep a track of the history we can use either of the two methods: \\\n",
        "1. wrapping the code block in with torch.no_grad()\n",
        "2. using .detach() to get a new tensor with same content but no gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTJJ6R9_C46G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d58ad122-f1b9-4475-e164-2f4e6d15aa76"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x**2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print((x**2).requires_grad)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXpybWbQEpGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "eac9c43e-5b07-42a3-b218-b7710c749ffc"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCwhAEKBE2vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}